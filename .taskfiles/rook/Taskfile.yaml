---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

x-env: &env
  disk: "{{.disk}}"
  job: "{{.job}}"
  node: "{{.node}}"

vars:
  ROOK_SCRIPTS_DIR: "{{.ROOT_DIR}}/.taskfiles/Rook/scripts"
  ROOK_TEMPLATES_DIR: "{{.ROOT_DIR}}/.taskfiles/Rook/templates"
  # Default cluster context since you only have one
  CLUSTER: "{{.cluster | default \"admin@kubernetes\"}}"
  # Node mappings - hostnames for Kubernetes, IPs for Talos
  TALOS_NODE_NAMES: [talos-1, talos-2, talos-3]
  TALOS_NODE_IPS: [10.10.10.11, 10.10.10.12, 10.10.10.13]
  # Disk mappings for easier maintenance
  TALOS_1_DISKS:
    - /dev/disk/by-id/nvme-Lexar_SSD_NM790_4TB_NME714W100393P2202
    - /dev/disk/by-id/nvme-Samsung_SSD_980_PRO_2TB_S6B0NL0W412707M
  TALOS_2_DISKS:
    - /dev/disk/by-id/nvme-Samsung_SSD_990_PRO_2TB_S7KHNJ0WC55436E
    - /dev/disk/by-id/nvme-Lexar_SSD_NM790_4TB_NME714W101694P2202
  TALOS_3_DISKS:
    - /dev/disk/by-id/nvme-Samsung_SSD_980_PRO_2TB_S69ENL0TC06068B
    - /dev/disk/by-id/nvme-Lexar_SSD_NM790_4TB_NL1948W100519P2202

tasks:

  # Validation tasks
  validate-cluster:
    desc: Validate cluster is ready for operations
    cmds:
      - echo "üîç Validating cluster connectivity..."
      - kubectl cluster-info
      - kubectl get nodes
      - kubectl get pods -n rook-ceph || echo "‚ö†Ô∏è Rook Ceph not deployed yet"

  validate-templates:
    desc: Validate all required templates exist
    cmds:
      - echo "üîç Validating required templates..."
      - test -f {{.ROOK_TEMPLATES_DIR}}/WipeDiskJob.tmpl.yaml || echo "‚ùå Missing WipeDiskJob template"
      - test -f {{.ROOK_TEMPLATES_DIR}}/WipeDataJob.tmpl.yaml || echo "‚ùå Missing WipeDataJob template"
      - test -f {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh || echo "‚ùå Missing wait-for-job script"
      - echo "‚úÖ Template validation complete"

  validate-all:
    desc: Run all validation checks
    cmds:
      - task: validate-cluster
      - task: validate-templates

  # Backup tasks
  backup-rook-config:
    desc: Backup current Rook configuration
    cmds:
      - echo "üíæ Backing up Rook configuration..."
      - mkdir -p /tmp/rook-backups
      - |
        BACKUP_FILE="/tmp/rook-backups/rook-backup-$(date +%Y%m%d-%H%M%S).yaml"
        echo "# Rook Configuration Backup - $(date)" > $BACKUP_FILE
        echo "# Generated by Task automation" >> $BACKUP_FILE
        echo "---" >> $BACKUP_FILE
        kubectl get cephcluster -n rook-ceph -o yaml >> $BACKUP_FILE || echo "No CephCluster found"
        kubectl get cephblockpool -n rook-ceph -o yaml >> $BACKUP_FILE || echo "No CephBlockPool found"
        kubectl get cephfilesystem -n rook-ceph -o yaml >> $BACKUP_FILE || echo "No CephFilesystem found"
        kubectl get cephobjectstore -n rook-ceph -o yaml >> $BACKUP_FILE || echo "No CephObjectStore found"
        echo "‚úÖ Backup saved to $BACKUP_FILE"

  # Disk status checks
  check-disks:
    desc: Check disk status on all nodes
    cmds:
      - echo "üîç Checking disk status on all nodes..."
      - for: { var: TALOS_NODE_NAMES }
        task: check-node-disks
        vars:
          node: "{{.ITEM}}"

  check-node-disks:
    desc: Check disk status on specific node
    cmds:
      - echo "üîç Checking disks on {{.node}}..."
      - |
        kubectl debug node/{{.node}} -it --image=busybox -- sh -c "
          echo 'üìÄ Block devices:' &&
          ls -la /host/dev/nvme* /host/dev/sd* 2>/dev/null || echo 'No standard block devices found' &&
          echo 'üíæ Mount points:' &&
          df -h /host &&
          echo 'üóÇÔ∏è Disk by-id (first 10):' &&
          ls -la /host/dev/disk/by-id/ 2>/dev/null | head -10 || echo 'No disk by-id entries'
        " || echo "‚ùå Failed to check {{.node}}"

  # Alternative disk checking using Talos
  check-disks-talos:
    desc: Check disk status using Talos CLI
    cmds:
      - echo "üîç Checking disk status using Talos..."
      - for: { var: TALOS_NODE_IPS }
        task: check-node-disks-talos
        vars:
          node_ip: "{{.ITEM}}"

  check-node-disks-talos:
    desc: Check disk status on specific node using Talos
    cmds:
      - echo "üîç Checking disks on {{.node_ip}}..."
      - echo "üìÄ Available disk devices:"
      - talosctl -n {{.node_ip}} list /dev/disk/by-id/ | grep -E "nvme|scsi|ata" || echo "‚ùå No NVMe/SCSI/ATA devices found"
      - echo "üíæ Filesystem mounts:"
      - talosctl -n {{.node_ip}} mounts 2>/dev/null | grep -E "nvme|sd" || echo "‚ùå No relevant mounts found"
      - echo "üóÇÔ∏è Block devices:"
      - talosctl -n {{.node_ip}} list /dev/ | grep -E "sd[a-z]|nvme[0-9]" || echo "‚ùå No block devices found"

  validate-wipe:
    desc: Validate that disk wipe was successful
    cmds:
      - echo "üîç Validating disk wipe results..."
      - echo "üìã Checking for any remaining filesystem signatures..."
      - for: { var: TALOS_NODE_NAMES }
        task: check-wipe-node
        vars:
          node: "{{.ITEM}}"

  check-wipe-node:
    desc: Check wipe status on specific node
    cmds:
      - echo "üîç Checking wipe status on {{.node}}..."
      - |
        kubectl debug node/{{.node}} -it --image=busybox -- sh -c "
          echo 'üîç Checking for filesystem signatures on data disks...' &&
          for disk in /host/dev/nvme1n1 /host/dev/nvme0n1 /host/dev/nvme2n1; do
            if [ -e \$disk ]; then
              echo \"Checking \$disk:\" &&
              od -t x1 -N 512 \$disk 2>/dev/null | head -5 || echo \"Cannot read \$disk\" &&
              echo \"---\"
            fi
          done
        " || echo "‚ùå Failed to check {{.node}}"

  reset:
    desc: Reset Rook
    vars: &vars
      cluster: "{{.CLUSTER}}"
      disk: "{{.cephdisk}}"
      node: "{{.ITEM}}"
    cmds:
      - for: { var: cephnodes }
        task: reset-data
        vars: *vars
      - for: { var: cephnodes }
        task: reset-disk
        vars: *vars
    requires:
      vars: ["cephnodes", "cephdisk"]

  reset-disk:
    desc: Reset a rook disk on a node
    prompt: Reset rook disk '{{.disk}}' on '{{.node}}' in the '{{.CLUSTER}}' cluster ... continue?
    summary: |
      Args:
        disk: Disk to wipe (required)
        node: Node the disk is on (required)
    cmds:
      - envsubst < <(cat {{.ROOK_TEMPLATES_DIR}}/WipeDiskJob.tmpl.yaml) | kubectl apply -f -
      - bash {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh {{.job}} default
      - kubectl -n default wait job/{{.job}} --for condition=complete --timeout=1m
      - kubectl -n default logs job/{{.job}}
      - kubectl -n default delete job {{.job}}
    env: *env
    requires:
      vars: ["disk", "node"]
    vars:
      job: wipe-disk-{{.node}}-{{now | date "150405"}}
    preconditions:
      - test -f {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh
      - test -f {{.ROOK_TEMPLATES_DIR}}/WipeDiskJob.tmpl.yaml

  reset-data:
    desc: Reset rook data on a node
    prompt: Reset rook data on node '{{.node}}' in the '{{.CLUSTER}}' cluster ... continue?
    summary: |
      Args:
        node: Node the data is on (required)
    cmds:
      - envsubst < <(cat {{.ROOK_TEMPLATES_DIR}}/WipeDataJob.tmpl.yaml) | kubectl apply -f -
      - bash {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh {{.job}} default
      - kubectl -n default wait job/{{.job}} --for condition=complete --timeout=1m
      - kubectl -n default logs job/{{.job}}
      - kubectl -n default delete job {{.job}}
    env: *env
    requires:
      vars: ["node"]
    vars:
      job: wipe-data-{{.node}}-{{now | date "150405"}}
    preconditions:
      - test -f {{.ROOK_SCRIPTS_DIR}}/wait-for-job.sh
      - test -f {{.ROOK_TEMPLATES_DIR}}/WipeDataJob.tmpl.yaml

  # New simplified tasks for your specific setup
  wipe-all:
    desc: Wipe all disks and data for your Rook cluster
    prompt: This will wipe ALL Rook disks and data on ALL nodes. Are you sure?
    cmds:
      - task: wipe-talos-1
      - task: wipe-talos-2
      - task: wipe-talos-3
      - task: reset-all-data

  # Enhanced wipe-all with progress
  wipe-all-with-progress:
    desc: Wipe all with progress indicators
    prompt: This will wipe ALL Rook disks and data on ALL nodes with progress tracking. Are you sure?
    cmds:
      - echo "üîÑ Starting full cluster wipe..."
      - task: backup-rook-config
      - task: wipe-talos-1
      - echo "‚úÖ talos-1 complete (1/3)"
      - task: wipe-talos-2
      - echo "‚úÖ talos-2 complete (2/3)"
      - task: wipe-talos-3
      - echo "‚úÖ talos-3 complete (3/3)"
      - task: reset-all-data
      - echo "üéâ All operations completed!"

  wipe-talos-1:
    desc: Wipe all disks on talos-1
    cmds:
      - echo "üîÑ Wiping disks on talos-1..."
      - for: { var: TALOS_1_DISKS }
        task: reset-disk
        vars:
          node: talos-1
          disk: "{{.ITEM}}"
      - echo "‚úÖ talos-1 disk wipe complete"

  wipe-talos-2:
    desc: Wipe all disks on talos-2
    cmds:
      - echo "üîÑ Wiping disks on talos-2..."
      - for: { var: TALOS_2_DISKS }
        task: reset-disk
        vars:
          node: talos-2
          disk: "{{.ITEM}}"
      - echo "‚úÖ talos-2 disk wipe complete"

  wipe-talos-3:
    desc: Wipe all disks on talos-3
    cmds:
      - echo "üîÑ Wiping disks on talos-3..."
      - for: { var: TALOS_3_DISKS }
        task: reset-disk
        vars:
          node: talos-3
          disk: "{{.ITEM}}"
      - echo "‚úÖ talos-3 disk wipe complete"

  # Quick individual node tasks
  quick-wipe-1:
    desc: Quick wipe of talos-1 (disks + data)
    cmds:
      - task: wipe-talos-1
      - task: reset-data
        vars:
          node: talos-1

  quick-wipe-2:
    desc: Quick wipe of talos-2 (disks + data)
    cmds:
      - task: wipe-talos-2
      - task: reset-data
        vars:
          node: talos-2

  quick-wipe-3:
    desc: Quick wipe of talos-3 (disks + data)
    cmds:
      - task: wipe-talos-3
      - task: reset-data
        vars:
          node: talos-3

  # Enhanced status check task
  status:
    desc: Check the status of wipe jobs
    cmds:
      - echo "üîç Checking wipe job status..."
      - kubectl get jobs -A | grep -E "wipe-disk|wipe-data" || echo "‚úÖ No wipe jobs found"
      - kubectl get pods -A | grep -E "wipe-disk|wipe-data" || echo "‚úÖ No wipe pods found"

  # Enhanced cleanup task
  cleanup:
    desc: Clean up any leftover wipe jobs
    cmds:
      - echo "üßπ Cleaning up leftover wipe jobs..."
      - kubectl delete jobs -A -l job-type=wipe-disk || true
      - kubectl delete jobs -A -l job-type=wipe-data || true
      - kubectl delete jobs -A --field-selector status.successful=0 | grep -E "wipe-disk|wipe-data" || true
      - echo "‚úÖ Cleanup complete"

  # Emergency stop task
  emergency-stop:
    desc: Emergency stop all wipe operations
    prompt: This will force stop all running wipe operations. Continue?
    cmds:
      - echo "üõë Emergency stop initiated..."
      - kubectl delete jobs -A -l job-type=wipe-disk --force --grace-period=0 || true
      - kubectl delete jobs -A -l job-type=wipe-data --force --grace-period=0 || true
      - kubectl delete pods -A --field-selector status.phase=Running | grep -E "wipe-disk|wipe-data" || true
      - echo "üõë Emergency stop complete"

  # Health check task
  health-check:
    desc: Check cluster health after operations
    cmds:
      - echo "üè• Checking cluster health..."
      - echo "üìã Node status:"
      - kubectl get nodes -o wide
      - echo "üìã Rook Ceph pods:"
      - kubectl get pods -n rook-ceph || echo "‚ö†Ô∏è Rook Ceph not deployed"
      - echo "üìã Rook Ceph services:"
      - kubectl get svc -n rook-ceph || echo "‚ö†Ô∏è Rook Ceph services not found"
      - echo "üìã Storage classes:"
      - kubectl get storageclass | grep -E "rook|ceph" || echo "‚ö†Ô∏è No Rook/Ceph storage classes found"
      - echo "üìã Persistent volumes:"
      - kubectl get pv | grep -E "rook|ceph" || echo "‚ö†Ô∏è No Rook/Ceph persistent volumes found"
      - echo "üè• Health check complete"

  # Comprehensive pre-wipe validation
  pre-wipe-validation:
    desc: Comprehensive validation before wiping disks
    cmds:
      - echo "üîç Pre-wipe validation starting..."
      - task: validate-cluster
      - task: validate-templates
      - task: backup-rook-config
      - task: check-disks
      - echo "‚úÖ Pre-wipe validation complete"

  # Comprehensive post-wipe validation
  post-wipe-validation:
    desc: Comprehensive validation after wiping disks
    cmds:
      - echo "üîç Post-wipe validation starting..."
      - task: validate-wipe
      - task: status
      - task: cleanup
      - task: health-check
      - echo "‚úÖ Post-wipe validation complete"
