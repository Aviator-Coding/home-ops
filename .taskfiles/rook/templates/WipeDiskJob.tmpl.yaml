---
# IMPORTANT: This template uses ceph-volume to properly clear BlueStore labels.
# Standard tools (sgdisk, dd) do NOT clear BlueStore superblock metadata.
# See: .claude/skills/ceph-rook/references/osd-recovery.md
apiVersion: batch/v1
kind: Job
metadata:
  name: ${job}
  namespace: default
spec:
  ttlSecondsAfterFinished: 3600
  template:
    spec:
      automountServiceAccountToken: false
      restartPolicy: Never
      nodeName: ${node}
      hostPID: true
      hostNetwork: true
      containers:
        - name: main
          image: quay.io/ceph/ceph:v19.2.3
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "Wiping disk /host${disk} with ceph-volume..."
              # Use ceph-volume to properly clear BlueStore labels and LVM metadata
              ceph-volume lvm zap --destroy /host${disk} || echo "ceph-volume lvm zap failed, trying raw zap..."
              # Fallback to raw zap for non-LVM OSDs
              ceph-volume raw zap /host${disk} || echo "ceph-volume raw zap completed or not applicable"
              # Final cleanup: issue TRIM/discard to SSD and notify kernel
              blkdiscard /host${disk} || echo "blkdiscard not supported"
              partprobe /host${disk} || true
              echo "Disk wipe complete for /host${disk}"
          securityContext:
            privileged: true
          resources: {}
          volumeMounts:
            - name: host-dev
              mountPath: /host/dev
              mountPropagation: Bidirectional
      volumes:
        - name: host-dev
          hostPath:
            path: /dev
            type: Directory
