---
apiVersion: batch/v1
kind: Job
metadata:
  name: initial-ceph-backup
  namespace: rook-ceph
spec:
  template:
    spec:
      serviceAccountName: rook-ceph-backup
      containers:
        - name: backup
          image: bitnami/kubectl:latest
          command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail

              # Wait for cluster to be ready
              echo "Waiting for Ceph cluster to be ready..."
              kubectl wait --for=condition=Ready cephcluster/rook-ceph -n rook-ceph --timeout=600s

              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              BACKUP_DIR="/backup/initial-${BACKUP_DATE}"
              mkdir -p "${BACKUP_DIR}"

              echo "Creating initial backup after cluster creation at ${BACKUP_DATE}"

              # Wait a bit more for all components to be stable
              sleep 30

              # Backup Kubernetes secrets
              kubectl get secret rook-ceph-mon -n rook-ceph -o yaml > "${BACKUP_DIR}/rook-ceph-mon.yaml"
              kubectl get secret rook-ceph-admin-keyring -n rook-ceph -o yaml > "${BACKUP_DIR}/rook-ceph-admin-keyring.yaml"

              # Backup all Ceph-related secrets
              kubectl get secrets -n rook-ceph -l app=rook-ceph-mon -o yaml > "${BACKUP_DIR}/all-ceph-secrets.yaml"

              # Backup CephCluster configuration
              kubectl get cephcluster -n rook-ceph -o yaml > "${BACKUP_DIR}/cephcluster.yaml"

              # Backup monitor endpoints
              kubectl get configmap rook-ceph-mon-endpoints -n rook-ceph -o yaml > "${BACKUP_DIR}/mon-endpoints.yaml"

              # Get cluster FSID for documentation
              kubectl get secret rook-ceph-mon -n rook-ceph -o jsonpath='{.data.fsid}' | base64 -d > "${BACKUP_DIR}/cluster-fsid.txt"

              echo "CRITICAL: New cluster FSID is: $(cat ${BACKUP_DIR}/cluster-fsid.txt)"

              # Backup monitor data from nodes
              for node in talos-1 talos-2 talos-3; do
                MON_POD=$(kubectl get pods -n rook-ceph -l app=rook-ceph-mon --field-selector spec.nodeName=$node -o name | head -1)
                if [ -n "$MON_POD" ]; then
                  echo "Backing up monitor data from $node"
                  kubectl exec -n rook-ceph $MON_POD -- tar czf - /var/lib/rook/rook-ceph/ > "${BACKUP_DIR}/mon-${node}.tar.gz"
                fi
              done

              # Create initial backup manifest
              cat > "${BACKUP_DIR}/backup-manifest.yaml" <<EOF
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: initial-backup-${BACKUP_DATE}
                namespace: rook-ceph
                labels:
                  backup-type: ceph-metadata
                  backup-date: ${BACKUP_DATE}
                  backup-purpose: initial-cluster-backup
              data:
                backup-date: "${BACKUP_DATE}"
                cluster-fsid: "$(cat ${BACKUP_DIR}/cluster-fsid.txt)"
                backup-status: "completed"
                notes: "Initial backup after cluster creation - CRITICAL for recovery"
              EOF

              kubectl apply -f "${BACKUP_DIR}/backup-manifest.yaml"

              echo "Initial backup completed successfully!"
              echo "Cluster FSID: $(cat ${BACKUP_DIR}/cluster-fsid.txt)"
              echo "Backup location: ${BACKUP_DIR}"
          volumeMounts:
            - name: backup-storage
              mountPath: /backup
      volumes:
        - name: backup-storage
          persistentVolumeClaim:
            claimName: ceph-backup-pvc
      restartPolicy: Never
  backoffLimit: 3
