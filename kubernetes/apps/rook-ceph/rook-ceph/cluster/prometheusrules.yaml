---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ceph-custom-alerts
  namespace: rook-ceph
spec:
  groups:
    - name: ceph-reliability
      rules:
        - alert: CephMonitorMemoryHigh
          expr: |
            container_memory_working_set_bytes{namespace="rook-ceph",container="mon"}
            / container_spec_memory_limit_bytes{namespace="rook-ceph",container="mon"} > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Ceph monitor {{ $labels.pod }} memory usage above 80%"

        - alert: CephOSDRestarts
          expr: increase(kube_pod_container_status_restarts_total{namespace="rook-ceph",container="osd"}[1h]) > 3
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OSD {{ $labels.pod }} has restarted more than 3 times in 1 hour"

        - alert: CephOSDSlowOps
          expr: ceph_osd_slow_ops > 5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "OSD {{ $labels.ceph_daemon }} experiencing sustained slow operations"
            description: "OSD {{ $labels.ceph_daemon }} has {{ $value }} slow operations for more than 10 minutes."

        - alert: CephMonitorQuorumAtRisk
          expr: ceph_mon_quorum_status < 3
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Ceph monitor quorum has fewer than 3 members"

        # Capacity alerts
        - alert: CephClusterNearFull
          expr: |
            ceph_cluster_total_used_bytes / ceph_cluster_total_bytes > 0.75
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Ceph cluster storage usage above 75%"
            description: "Cluster is {{ $value | humanizePercentage }} full. Consider adding capacity or cleaning up data."

        - alert: CephClusterCriticallyFull
          expr: |
            ceph_cluster_total_used_bytes / ceph_cluster_total_bytes > 0.85
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Ceph cluster storage usage above 85%"
            description: "Cluster is {{ $value | humanizePercentage }} full. Immediate action required."

        - alert: CephOSDNearFull
          expr: |
            (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes) > 0.80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "OSD {{ $labels.ceph_daemon }} storage usage above 80%"
            description: "OSD {{ $labels.ceph_daemon }} is {{ $value | humanizePercentage }} full."

        - alert: CephOSDFull
          expr: |
            (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes) > 0.90
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "OSD {{ $labels.ceph_daemon }} storage usage above 90%"
            description: "OSD {{ $labels.ceph_daemon }} is {{ $value | humanizePercentage }} full. Critical!"

        # OSD down/health alerts
        - alert: CephOSDDown
          expr: ceph_osd_up == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Ceph OSD {{ $labels.ceph_daemon }} is down"
            description: "OSD {{ $labels.ceph_daemon }} has been down for more than 5 minutes."

        - alert: CephOSDOut
          expr: ceph_osd_in == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Ceph OSD {{ $labels.ceph_daemon }} is out of the cluster"
            description: "OSD {{ $labels.ceph_daemon }} is marked out and not participating in data placement."

        - alert: CephOSDFlapping
          expr: |
            changes(ceph_osd_up[10m]) > 4
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: "OSD {{ $labels.ceph_daemon }} is flapping"
            description: "OSD {{ $labels.ceph_daemon }} has changed state more than 4 times in 10 minutes."

    - name: ceph-pg-health
      rules:
        # Placement Group Health Alerts
        - alert: CephPGDegraded
          expr: ceph_pg_degraded > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Ceph placement groups are degraded"
            description: "{{ $value }} placement groups are in degraded state. Data is still accessible but not fully replicated."

        - alert: CephPGInactive
          expr: ceph_pg_inactive > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Ceph placement groups are inactive"
            description: "{{ $value }} placement groups are inactive. Data may be inaccessible."

        - alert: CephPGInconsistent
          expr: ceph_pg_inconsistent > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Ceph placement groups are inconsistent"
            description: "{{ $value }} placement groups have inconsistent data. Run 'ceph pg repair' after investigation."

        - alert: CephPGRecoveryFull
          expr: ceph_pg_recovery_full > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Ceph recovery blocked - cluster full"
            description: "Placement group recovery is blocked because the cluster is too full."

    - name: ceph-disk-health
      rules:
        # Disk Health Prediction Alerts
        - alert: CephDiskPredictedFailure
          expr: ceph_device_life_expectancy_min < 7
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Disk failure predicted for {{ $labels.device }}"
            description: "Device {{ $labels.device }} on {{ $labels.host }} is predicted to fail within {{ $value }} days. Plan replacement."

        - alert: CephDiskPredictedFailureImminent
          expr: ceph_device_life_expectancy_min < 1 and ceph_device_life_expectancy_min >= 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Disk failure imminent for {{ $labels.device }}"
            description: "Device {{ $labels.device }} on {{ $labels.host }} is predicted to fail within 24 hours. Replace immediately."

    - name: ceph-crash-detection
      rules:
        # Crash Detection Alert
        - alert: CephCrashesDetected
          expr: ceph_health_detail{name="RECENT_CRASH"} == 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Ceph daemon crash detected"
            description: "A Ceph daemon has crashed recently. Run 'ceph crash ls-new' to see details and 'ceph crash archive-all' after review."
