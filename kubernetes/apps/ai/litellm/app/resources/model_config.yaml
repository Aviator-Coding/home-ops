router_model_list:
  - router_model_name: vision-router
    model_group: vision_tasks
    litellm_params:
      temperature: 0.5 # Temperature controls creativity: 0.0 = deterministic, 1.0 = creative
    metadata:
      label: "Vision Assistant"
      capabilities: ["chat", "vision"]

  - router_model_name: code-router
    model_group: code_pipeline
    litellm_params:
      temperature: 0.2 # Temperature controls creativity: 0.0 = deterministic, 1.0 = creative
    metadata:
      label: "Code Assistant"
      capabilities: ["chat", "code"]

  - router_model_name: education-router
    model_group: education_pipeline
    litellm_params:
      temperature: 0.6 # Temperature controls creativity: 0.0 = deterministic, 1.0 = creative
    metadata:
      label: "Education Assistant"
      capabilities: ["chat", "reasoning", "tutoring"]

model_list:
  # Actual Model Definitions
  # Each entry maps to a real provider + model pairing
  # Models may define aliases for routing/fallback via `model_groups`

  # Anthropic Models

  - model_name: "Anthopic: claude-4.5-opus"
    provider: anthropic
    litellm_params:
      model: anthropic/claude-opus-4-5
    api_key: ${ANTHROPIC_API_KEY}
    model_info:
      max_tokens: 200000
      input_cost_per_1k_tokens: 0.0008
      output_cost_per_1k_tokens: 0.004
      supports_vision: true
      supports_function_calling: true
    aliases: [general_anthropic, vision_anthropic]

  - model_name: "Anthopic: claude-4.5-sonnet"
    provider: anthropic
    litellm_params:
      model: anthropic/claude-sonnet-4-5
    api_key: ${ANTHROPIC_API_KEY}
    model_info:
      max_tokens: 200000
      input_cost_per_1k_tokens: 0.0008
      output_cost_per_1k_tokens: 0.004
      supports_vision: true
      supports_function_calling: true
    aliases: [general_anthropic, vision_anthropic]

  - model_name: "Anthopic: claude-4.5-haiku"
    provider: anthropic
    litellm_params:
      model: anthropic/claude-haiku-4-5
    api_key: ${ANTHROPIC_API_KEY}
    model_info:
      max_tokens: 200000
      input_cost_per_1k_tokens: 0.0008
      output_cost_per_1k_tokens: 0.004
      supports_vision: true
      supports_function_calling: true
    aliases: [general_anthropic, vision_anthropic]

  - model_name: "Anthopic: claude-3.5-haiku"
    provider: anthropic
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
    api_key: ${ANTHROPIC_API_KEY}
    model_info:
      max_tokens: 200000
      input_cost_per_1k_tokens: 0.0008
      output_cost_per_1k_tokens: 0.004
      supports_vision: true
      supports_function_calling: true
    aliases: [general_anthropic, vision_anthropic]

  # Deepseek Models
  - model_name: "Deepseek: Chat"
    provider: deepseek
    litellm_params:
      model: deepseek/deepseek-chat
    api_key: ${DEEPSEEK_API_KEY}
    model_info:
      max_tokens: 65536
      input_cost_per_1k_tokens: 0.00027
      output_cost_per_1k_tokens: 0.0011
      supports_function_calling: true
    aliases: [general_deepseek]

  - model_name: "Deepseek: Coder"
    provider: deepseek
    litellm_params:
      model: deepseek/deepseek-coder
    api_key: ${DEEPSEEK_API_KEY}
    model_info:
      max_tokens: 128000
      input_cost_per_1k_tokens: 0.00014
      output_cost_per_1k_tokens: 0.00028
      supports_function_calling: true
    aliases: [code_deepseek]

  - model_name: "Deepseek: Reasoner"
    provider: deepseek
    litellm_params:
      model: deepseek/deepseek-reasoner
    api_key: ${DEEPSEEK_API_KEY}
    model_info:
      max_tokens: 65536
      input_cost_per_1k_tokens: 0.00055
      output_cost_per_1k_tokens: 0.00219
      supports_function_calling: true
      supports_reasoning: true
    aliases: [reasoner_deepseek]

  # Open AI Models
  - model_name: "OpenAI: text-embedding-3-small"
    provider: openai
    litellm_params:
      model: openai/text-embedding-3-small
    api_key: ${OPENAI_API_KEY}
    model_info:
      mode: embedding # Required for embeddings
      max_tokens: 8192
      input_cost_per_1k_tokens: 0.00002
    aliases: [embedding_openai]

  - model_name: "OpenAI: gpt-5.2"
    provider: openai
    litellm_params:
      model: openai/gpt-5.2
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 400000
      input_cost_per_1k_tokens: 1.75
      output_cost_per_1k_tokens: 14.00
      supports_vision: true
      supports_reasoning: true
      supports_function_calling: true
    aliases: [general_openai_5, vision_openai_5, education_5]

  - model_name: "OpenAI: gpt-5"
    provider: openai
    litellm_params:
      model: openai/gpt-5
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 400000
      input_cost_per_1k_tokens: 0.00125
      output_cost_per_1k_tokens: 0.01
      supports_vision: true
      supports_reasoning: true
      supports_function_calling: true
    aliases: [general_openai_5, vision_openai_5, education_5]

  - model_name: "OpenAI: gpt-5-mini"
    provider: openai
    litellm_params:
      model: openai/gpt-5-mini
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 400000
      input_cost_per_1k_tokens: 0.00025
      output_cost_per_1k_tokens: 0.002
      supports_vision: true
      supports_reasoning: true
      supports_function_calling: true
    aliases: [gpt-4.1, general_openai, vision_openai]

  - model_name: "OpenAI: gpt-5-nano"
    provider: openai
    litellm_params:
      model: openai/gpt-5-nano
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 400000
      input_cost_per_1k_tokens: 0.00005
      output_cost_per_1k_tokens: 0.0004
      supports_vision: true
      supports_reasoning: true
      supports_function_calling: true
    aliases: [general_openai_5, vision_openai_5, education_5]

  - model_name: "OpenAI: gpt-5-codex"
    provider: openai
    litellm_params:
      model: openai/gpt-5-codex
    api_key: ${OPENAI_API_KEY}
    model_info:
      mode: chat
      max_tokens: 272000
      input_cost_per_1k_tokens: 0.00125
      output_cost_per_1k_tokens: 0.01
      supports_vision: true
      supports_function_calling: true
    aliases: [general_openai_5, vision_openai_5, education_5]

  - model_name: "OpenAI: gpt-4.1-mini"
    provider: openai
    litellm_params:
      model: openai/gpt-4.1-mini
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0004
      output_cost_per_1k_tokens: 0.0016
      supports_vision: true
      supports_function_calling: true
    aliases: [gpt-4.1, general_openai, vision_openai]

  - model_name: "OpenAI: gpt-40-mini-tts"
    provider: openai
    litellm_params:
      model: openai/gpt-4o-mini-tts-2025-03-20
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0004
      output_cost_per_1k_tokens: 0.0016
      supports_vision: true
      supports_function_calling: true
    aliases: [general_openai_4o, general_openai, vision_openai]

  - model_name: "OpenAI: gpt-4o"
    provider: openai
    litellm_params:
      model: openai/gpt-4o
    api_key: ${OPENAI_API_KEY}
    model_info:
      max_tokens: 128000
      input_cost_per_1k_tokens: 0.0025
      output_cost_per_1k_tokens: 0.01
      supports_vision: true
      supports_function_calling: true
    aliases: [general_openai_4o, vision_openai_4o, education_4o]

  - model_name: "OpenAI: Whisper-1"
    provider: openai
    litellm_params:
      model: openai/whisper-1
    api_key: ${OPENAI_API_KEY}
    model_info:
      id: whisper-1
      mode: audio_transcription
      input_cost_per_minute: 0.006 # Assuming cost is per minute for audio
    aliases: [whisper-1]

  # TogetherAI Models

  - model_name: "TogetherAI: Qwen2.5-Coder-32B-Instruct"
    provider: togetherai
    litellm_params:
      model: together_ai/Qwen/Qwen2.5-Coder-32B-Instruct
    api_key: ${TOGETHER_API_KEY}
    model_info:
      id: Qwen2.5-Coder-32B-Instruct
      mode: chat
      input_cost_per_1k_tokens: 0.0008
      output_cost_per_1k_tokens: 0.0008
      max_tokens: 16384
    aliases: [code_together]

  # Groq Models
  - model_name: "Groq: Meta llama-3.3-70b-versatile"
    provider: groq
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      drop_params: true
    api_key: ${GROQ_API_KEY}
    model_info:
      max_tokens: 128000
      input_cost_per_1k_tokens: 0.00059
      output_cost_per_1k_tokens: 0.00079
      supports_function_calling: true
    aliases: [general_groq, code_groq]

  - model_name: "Groq: Meta gemma2-9b-it"
    provider: groq
    litellm_params:
      model: groq/gemma2-9b-it
      drop_params: true
    api_key: ${GROQ_API_KEY}
    model_info:
      max_tokens: 8192
      input_cost_per_1k_tokens: 0.0002
      output_cost_per_1k_tokens: 0.0002
    aliases: [general_groq]

  - model_name: "Groq: Meta llama-3.1-8b-instant"
    provider: groq
    litellm_params:
      model: groq/llama-3.1-8b-instant
      drop_params: true
    api_key: ${GROQ_API_KEY}
    model_info:
      max_tokens: 128000
      input_cost_per_1k_tokens: 0.00005
      output_cost_per_1k_tokens: 0.00008
      supports_function_calling: true
    aliases: [code_groq]

  - model_name: "Groq: Meta compound-mini [No-Limits]"
    provider: groq
    litellm_params:
      model: groq/compound-mini
      drop_params: true
    api_key: ${GROQ_API_KEY}
    aliases: [general_groq, reasoning_groq]

  - model_name: "Groq: Meta compound [No-Limits]"
    provider: groq
    litellm_params:
      model: groq/compound
      drop_params: true
    api_key: ${GROQ_API_KEY}
    aliases: [general_groq, reasoning_groq]

  - model_name: "Groq: gpt-oss-120b"
    provider: groq
    litellm_params:
      model: openai/gpt-oss-120b
      drop_params: true
    api_key: ${GROQ_API_KEY}
    aliases: [general_groq, code_groq]

  - model_name: "Groq: gpt-oss-20b"
    provider: groq
    litellm_params:
      model: openai/gpt-oss-20b
      drop_params: true
    api_key: ${GROQ_API_KEY}
    aliases: [general_groq, code_groq]

  - model_name: "Groq: qwen3-32b"
    provider: groq
    litellm_params:
      model: qwen/qwen3-32b
      drop_params: true
    api_key: ${GROQ_API_KEY}
    aliases: [general_groq, code_groq]

  # xAI Models
  - model_name: "xAI: Grok 4 Fast"
    provider: xai
    litellm_params:
      model: xai/grok-4-fast-reasoning
      api_base: "https://api.x.ai/v1" # Required endpoint for xAI
    api_key: ${XAI_API_KEY}
    model_info:
      max_tokens: 2000000
      input_cost_per_1k_tokens: 0.0002
      output_cost_per_1k_tokens: 0.0005
      supports_web_search: true
      supports_reasoning: true
      supports_function_calling: true
    aliases: [code_grok]

  - model_name: "xAI: grok-3-mini-beta"
    provider: xai
    litellm_params:
      model: xai/grok-3-mini-beta
      api_base: "https://api.x.ai/v1" # Required endpoint for xAI
    api_key: ${XAI_API_KEY}
    model_info:
      max_tokens: 131072
      input_cost_per_1k_tokens: 0.0003
      output_cost_per_1k_tokens: 0.0005
      supports_web_search: true
      supports_reasoning: true
      supports_function_calling: true
    aliases: [general_xai, vision_xai]

  # Latest Gemini 3.0 Models (Current Generation)
  - model_name: "Gemini: 3.0 Pro"
    litellm_params:
      model: gemini/gemini-3-pro-preview
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00125
      output_cost_per_1k_tokens: 0.01
      supports_vision: true
      supports_web_search: true
      supports_reasoning: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [general_gemini, reasoning_gemini, vision_gemini]

  # Gemini Models
  # Latest Gemini 2.5 Models (Current Generation)
  - model_name: "Gemini: 2.5 Pro"
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00125
      output_cost_per_1k_tokens: 0.01
      supports_vision: true
      supports_web_search: true
      supports_reasoning: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [general_gemini, reasoning_gemini, vision_gemini]

  - model_name: "Gemini: 2.5 Flash"
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0003
      output_cost_per_1k_tokens: 0.0025
      supports_vision: true
      supports_web_search: true
      supports_url_context: true
      supports_reasoning: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [general_gemini_fast, vision_gemini_fast]

  - model_name: "Gemini: 2.5 Flash Lite"
    litellm_params:
      model: gemini/gemini-2.5-flash-lite
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0001
      output_cost_per_1k_tokens: 0.0004
      supports_vision: true
      supports_web_search: true
      supports_url_context: true
      supports_reasoning: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [general_gemini_lite]

  - model_name: "Gemini: 2.5 Flash Image Preview"
    litellm_params:
      model: gemini/gemini-2.5-flash-image-preview
      api_key: ${GEMINI_API_KEY}
    model_info:
      mode: image_generation
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0003
      output_cost_per_image: 0.03 # Assuming cost is per image
      supports_vision: true
      supports_web_search: true
      supports_url_context: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [image_gen_gemini]

  - model_name: "Gemini: 2.5 Flash Preview TTS"
    litellm_params:
      model: gemini/gemini-2.5-flash-preview-tts
      api_key: ${GEMINI_API_KEY}
    model_info:
      mode: tts
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00015
      output_cost_per_1k_tokens: 0.0006
      supports_vision: true
      supports_web_search: true
      supports_reasoning: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [gemini-tts-flash]

  - model_name: "Gemini: 2.5 Pro Preview TTS"
    litellm_params:
      model: gemini/gemini-2.5-pro-preview-tts
      api_key: ${GEMINI_API_KEY}
    model_info:
      mode: tts
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00125
      output_cost_per_1k_tokens: 0.01
      supports_vision: true
      supports_web_search: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [gemini-tts-pro]

  # Gemini 2.0 Models
  - model_name: "Gemini: 2.0 Flash"
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0001
      output_cost_per_1k_tokens: 0.0004
      supports_vision: true
      supports_web_search: true
      supports_url_context: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [general_gemini_2, vision_gemini_2]

  - model_name: "Gemini: 2.0 Flash Preview Image Generation"
    litellm_params:
      model: gemini/gemini-2.0-flash-preview-image-generation
      api_key: ${GEMINI_API_KEY}
    model_info:
      mode: image_generation
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.0001
      output_cost_per_1k_tokens: 0.0004
      supports_vision: true
      supports_web_search: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [gemini-image-gen-2]

  - model_name: "Gemini: 2.0 Flash Lite"
    litellm_params:
      model: gemini/gemini-2.0-flash-lite
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00007
      output_cost_per_1k_tokens: 0.0003
      supports_vision: true
      supports_web_search: true
      supports_function_calling: true
    aliases: [general_gemini_2_lite]

  - model_name: "Gemini: 2.0 Flash Live"
    litellm_params:
      model: gemini/gemini-2.0-flash-live-001
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00035
      output_cost_per_1k_tokens: 0.0015
      supports_vision: true
      supports_web_search: true
      supports_url_context: true
      supports_reasoning: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [live_gemini_2]

  # Gemini 1.5 Models (Still Supported)
  - model_name: "Gemini: 1.5 Flash"
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 1000000
      input_cost_per_1k_tokens: 0.00007
      output_cost_per_1k_tokens: 0.0003
      supports_vision: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [general_gemini_legacy, vision_gemini_legacy]

  - model_name: "Gemini: 1.5 Pro"
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: ${GEMINI_API_KEY}
    model_info:
      max_tokens: 2097152
      input_cost_per_1k_tokens: 0.0035
      output_cost_per_1k_tokens: 0.0105
      supports_vision: true
      supports_function_calling: true
      disable_background_health_check: true
    aliases: [reasoning_gemini_legacy]

  # Specialized Gemini Models
  - model_name: "Gemini: Imagen 3"
    litellm_params:
      model: gemini/imagen-3
      api_key: ${GEMINI_API_KEY}
    model_info:
      disable_background_health_check: true
    aliases: [image_gen_imagen]

  - model_name: "Gemini: Veo 3"
    litellm_params:
      model: gemini/veo-3
      api_key: ${GEMINI_API_KEY}
    model_info:
      disable_background_health_check: true
    aliases: [video_gen_gemini]

  - model_name: "Gemini: Veo 3 Fast"
    litellm_params:
      model: gemini/veo-3-fast
      api_key: ${GEMINI_API_KEY}
    model_info:
      disable_background_health_check: true
    aliases: [video_gen_gemini_fast]

  # Mistral

  - model_name: mistral-large-latest
    litellm_params:
      model: mistral/mistral-large-latest
      api_key: ${MISTRAL_API_KEY}

  - model_name: mistral-medium-latest
    litellm_params:
      model: mistral/mistral-medium-latest
      api_key: ${MISTRAL_API_KEY}

  - model_name: mistral-small-latest
    litellm_params:
      model: mistral/mistral-small-latest
      api_key: ${MISTRAL_API_KEY}

  - model_name: ministral-8b-latest
    litellm_params:
      model: mistral/ministral-8b-latest
      api_key: ${MISTRAL_API_KEY}

  - model_name: ministral-3b-latest
    litellm_params:
      model: mistral/ministral-3b-latest
      api_key: ${MISTRAL_API_KEY}
# Fallback Groups
# Each group is a list of model aliases (not full names!)
# These groups are referenced by `router_model_list` virtual models

model_groups:
  code_pipeline:
    - code_groq # Fast + cheap reasoning for code
    - code_deepseek # Strong code reasoning
    - code_together # Big context + strong code skills
    - general_gemini # Gemini 2.5 Pro for code reasoning
    - code_grok # xAI Grok 4 Fast for code reasoning

  general_tasks:
    - general_openai_4o # GPT-4o – multilingual, expressive
    - general_openai # GPT-4.1 Mini – strong balance of capability and cost
    - general_anthropic # Claude 3.5 – ideal for thoughtful reasoning and tone
    - general_gemini # Gemini 2.5 Pro – strong multimodal reasoning
    - general_gemini_fast # Gemini 2.5 Flash – fast and efficient
    - general_xai # Grok 3 – experimental fallback
    - general_deepseek # DeepSeek Chat – strong reasoning
    - general_groq # Groq models – fast and efficient general tasks

  vision_tasks:
    - vision_openai_4o # GPT-4o – best visual model for accuracy
    - vision_gemini # Gemini 2.5 Pro – excellent multimodal capabilities
    - vision_gemini_fast # Gemini 2.5 Flash – fast vision processing
    - vision_openai # GPT-4.1 Mini – vision-capable fallback
    - vision_anthropic # Claude 3.5 – vision-capable fallback
    - vision_xai # Grok 3 – visual reasoning backup

  education_pipeline:
    - education_4o # GPT-4o – ideal for tutoring with language diversity
    - general_gemini # Gemini 2.5 Pro – excellent for educational content
    - general_openai # GPT-4.1 Mini – ideal for tutoring
    - general_anthropic # Claude 3.5 – empathetic and capable in tutoring
    - general_xai # Grok 3 – fallback if others fail
    - reasoner_deepseek # DeepSeek Reasoner – strong reasoning for educational tasks
    - reasoning_groq # Groq compound models – fast reasoning for education

  # New specialized model groups for Gemini capabilities
  audio_tasks:
    - gemini-tts-flash # Gemini 2.5 Flash TTS
    - gemini-tts-pro # Gemini 2.5 Pro TTS

  image_generation:
    - image_gen_gemini # Gemini 2.5 Flash Image Preview
    - gemini-image-gen-2 # Gemini 2.0 Flash Image Generation
    - image_gen_imagen # Imagen 3

  live_interaction:
    - live_gemini # Gemini 2.5 Flash Live
    - live_gemini_2 # Gemini 2.0 Flash Live

  video_generation:
    - video_gen_gemini # Veo 3
    - video_gen_gemini_fast # Veo 3 Fast

  no_limits:
    - reasoning_groq # Groq compound models with no limits

  reasoning_tasks:
    - reasoner_deepseek # Best for complex reasoning
    - reasoning_groq # No-limits compound models
    - general_gemini # Strong reasoning capabilities
    - general_openai_5 # GPT-5 for advanced reasoning
    - general_openai_4o # GPT-4o fallback

  cost_efficient:
    - general_groq # Fastest & cheapest
    - general_deepseek # Good balance
    - general_gemini_fast # Fast Gemini
    - general_openai # GPT-4.1 Mini
